{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as f\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多分类问题 (利用softmax函数进行多分类)\n",
    "\n",
    "## Softmax Layer Theory\n",
    "\n",
    "Suppose $Z^l \\in \\mathbb{R}^K$ is the output of the last linear layer, the softmax function is defined as:\n",
    "\n",
    "$$ P(y = i) = \\frac{e^{Z_i}}{\\sum_{j=1}^Ke^{Z_j}} , i \\in \\{0, \\cdots, K-1 \\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy in numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9729189131256584\n"
     ]
    }
   ],
   "source": [
    "y = np.array([1, 0, 0])\n",
    "z = np.array([0.2, 0.1, -0.1])\n",
    "y_pred = np.exp(z) / np.sum(np.exp(z))\n",
    "loss = np.sum(-y * np.log(y_pred))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9729)\n"
     ]
    }
   ],
   "source": [
    "y = torch.LongTensor([0])\n",
    "z = torch.FloatTensor([[0.2, 0.1, -0.1]])\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "loss = criterion(z, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-1 Prepare Dataset\n",
    "\n",
    "Note the use of `transforms.ToTensor()` to convert the images from PIL format to Tensor format.\n",
    "$$ \\mathbb Z^{28 \\times 28}, pixel \\in \\{0, \\cdots, 255 \\} \\rightarrow \\mathbb Z^{1 \\times 28 \\times 28}, pixel \\in [0, 1] $$\n",
    "\n",
    "And we use `transforms.Normalize()` to normalize the data in this way:\n",
    "$$ Pixel_{norm} = \\frac{Pixel_{origin} - mean}{std} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                      # Convert PIL image to PyTorch Tensor.\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize using the mean value and std loss value\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./dataset/mnist', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./dataset/mnist', train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-2 Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(28 * 28, 512)\n",
    "        self.layer2 = torch.nn.Linear(512, 256)\n",
    "        self.layer3 = torch.nn.Linear(256, 128)\n",
    "        self.layer4 = torch.nn.Linear(128, 64)\n",
    "        self.layer5 = torch.nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = f.relu(self.layer1(x))\n",
    "        x = f.relu(self.layer2(x))\n",
    "        x = f.relu(self.layer3(x))\n",
    "        x = f.relu(self.layer4(x))\n",
    "        return self.layer5(x)\n",
    "    \n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-3 Define Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)    # 最后一个参数是冲量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-4 Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 2.293\n",
      "[1,   200] loss: 2.245\n",
      "[1,   300] loss: 2.053\n",
      "[1,   400] loss: 1.368\n",
      "[1,   500] loss: 0.748\n",
      "[1,   600] loss: 0.550\n",
      "[1,   700] loss: 0.502\n",
      "[1,   800] loss: 0.425\n",
      "[1,   900] loss: 0.407\n",
      "[2,   100] loss: 0.343\n",
      "[2,   200] loss: 0.329\n",
      "[2,   300] loss: 0.299\n",
      "[2,   400] loss: 0.285\n",
      "[2,   500] loss: 0.280\n",
      "[2,   600] loss: 0.269\n",
      "[2,   700] loss: 0.231\n",
      "[2,   800] loss: 0.233\n",
      "[2,   900] loss: 0.230\n",
      "[3,   100] loss: 0.190\n",
      "[3,   200] loss: 0.189\n",
      "[3,   300] loss: 0.188\n",
      "[3,   400] loss: 0.177\n",
      "[3,   500] loss: 0.176\n",
      "[3,   600] loss: 0.169\n",
      "[3,   700] loss: 0.160\n",
      "[3,   800] loss: 0.156\n",
      "[3,   900] loss: 0.158\n",
      "[4,   100] loss: 0.139\n",
      "[4,   200] loss: 0.140\n",
      "[4,   300] loss: 0.127\n",
      "[4,   400] loss: 0.129\n",
      "[4,   500] loss: 0.128\n",
      "[4,   600] loss: 0.128\n",
      "[4,   700] loss: 0.122\n",
      "[4,   800] loss: 0.110\n",
      "[4,   900] loss: 0.124\n",
      "[5,   100] loss: 0.107\n",
      "[5,   200] loss: 0.110\n",
      "[5,   300] loss: 0.094\n",
      "[5,   400] loss: 0.093\n",
      "[5,   500] loss: 0.104\n",
      "[5,   600] loss: 0.098\n",
      "[5,   700] loss: 0.107\n",
      "[5,   800] loss: 0.093\n",
      "[5,   900] loss: 0.082\n",
      "[6,   100] loss: 0.081\n",
      "[6,   200] loss: 0.083\n",
      "[6,   300] loss: 0.073\n",
      "[6,   400] loss: 0.086\n",
      "[6,   500] loss: 0.074\n",
      "[6,   600] loss: 0.082\n",
      "[6,   700] loss: 0.075\n",
      "[6,   800] loss: 0.075\n",
      "[6,   900] loss: 0.081\n",
      "[7,   100] loss: 0.063\n",
      "[7,   200] loss: 0.063\n",
      "[7,   300] loss: 0.065\n",
      "[7,   400] loss: 0.055\n",
      "[7,   500] loss: 0.067\n",
      "[7,   600] loss: 0.067\n",
      "[7,   700] loss: 0.060\n",
      "[7,   800] loss: 0.066\n",
      "[7,   900] loss: 0.062\n",
      "[8,   100] loss: 0.056\n",
      "[8,   200] loss: 0.046\n",
      "[8,   300] loss: 0.054\n",
      "[8,   400] loss: 0.052\n",
      "[8,   500] loss: 0.047\n",
      "[8,   600] loss: 0.054\n",
      "[8,   700] loss: 0.057\n",
      "[8,   800] loss: 0.046\n",
      "[8,   900] loss: 0.053\n",
      "[9,   100] loss: 0.042\n",
      "[9,   200] loss: 0.043\n",
      "[9,   300] loss: 0.032\n",
      "[9,   400] loss: 0.041\n",
      "[9,   500] loss: 0.048\n",
      "[9,   600] loss: 0.037\n",
      "[9,   700] loss: 0.040\n",
      "[9,   800] loss: 0.043\n",
      "[9,   900] loss: 0.042\n",
      "[10,   100] loss: 0.027\n",
      "[10,   200] loss: 0.031\n",
      "[10,   300] loss: 0.030\n",
      "[10,   400] loss: 0.035\n",
      "[10,   500] loss: 0.030\n",
      "[10,   600] loss: 0.040\n",
      "[10,   700] loss: 0.033\n",
      "[10,   800] loss: 0.036\n",
      "[10,   900] loss: 0.037\n",
      "Accuracy on test set:  97.6100%\n"
     ]
    }
   ],
   "source": [
    "def train(epoch: int):\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (inputs, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 100 == 99:\n",
    "            print(f'[{epoch + 1}, {batch_idx + 1:5d}] loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for (images, labels) in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f'Accuracy on test set: {100 * correct / total: .4f}%')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for epoch in range(10):\n",
    "        train(epoch)\n",
    "        if epoch % 10 == 9:\n",
    "            test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作业\n",
    "\n",
    "- [Cross Entropy Loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) 和 [NLL Loss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss) 的区别\n",
    "- 尝试理解: Cross Entropy Loss $\\Leftrightarrow$ Log Softmax + NLL Loss\n",
    "- 完成Kaggle中的[Otto Group Product Classification任务](https://www.kaggle.com/c/otto-group-product-classification-challenge/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding\n",
    "\n",
    "Cross Entropy Loss 和 NLL Loss 都是常用的分类任务的损失函数，它们的区别在于 Cross Entropy Loss 内部包含了 softmax 和 log 操作，而 NLL Loss 只是对输入的对数概率向量和目标标签进行负对数似然计算¹²。也就是说，Cross Entropy Loss = NLL Loss + softmax + log。因此，如果使用 Cross Entropy Loss 作为损失函数，神经网络的最后一层就不需要加入 softmax 或者 log softmax，而如果使用 NLL Loss，就需要在最后一层加入 log softmax³⁴。下面是一个简单的例子，说明两种损失函数的计算过程：\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 随机生成一个神经网络的最后一层，3行4列，那就是有4个标签\n",
    "input = torch.randn(3, 4)\n",
    "# input的第一行设置为标签1，第二行为标签0, 第三行为标签2\n",
    "label = torch.tensor([1, 0, 2])\n",
    "\n",
    "# 定义损失函数为NLLLoss\n",
    "loss = nn.NLLLoss()\n",
    "# 定义log softmax函数，也就是将input中的每一行转化为带有负号的数字\n",
    "m = nn.LogSoftmax(dim=1)\n",
    "# 计算损失，损失就是一个值。\n",
    "loss_value = loss(m(input), label)\n",
    "print(loss_value) # tensor(1.7075)\n",
    "\n",
    "# 定义损失函数为CrossEntropyLoss\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# 计算损失，损失就是一个值。\n",
    "loss_value = loss(input, label)\n",
    "print(loss_value) # tensor(1.7075)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "可以看到，两种损失函数的结果是一样的，只是 CrossEntropyLoss 不需要额外的 log softmax 操作。⁵\n",
    "\n",
    "Source: Conversation with Bing, 2/21/2024 \\\n",
    "(1) 神经网络中NLLLoss和CrossEntropyLoss的快速理解 - 知乎. https://zhuanlan.zhihu.com/p/589631793. \\\n",
    "(2) 详解pytorch的损失函数：NLLLoss()和CrossEntropyLoss() - 知乎. https://zhuanlan.zhihu.com/p/570118948. \\\n",
    "(3) 详解pytorch的损失函数：NLLLoss()和CrossEntropyLoss() - 知乎. https://bing.com/search?q=Cross+Entropy+Loss+%e5%92%8c+NLL+Loss+%e7%9a%84%e5%8c%ba%e5%88%ab. \\\n",
    "(4) pytorch中F.cross_entropy和F.nll_loss的区别-CSDN博客. https://blog.csdn.net/code_plus/article/details/115481575. \\\n",
    "(5) pytorch中CrossEntropyLoss和NLLLoss的区别与联系 - CSDN博客. https://blog.csdn.net/qq_25105061/article/details/107381316."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vpteam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
