{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as f\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多分类问题 (利用softmax函数进行多分类)\n",
    "\n",
    "## Softmax Layer Theory\n",
    "\n",
    "Suppose $Z^l \\in \\mathbb{R}^K$ is the output of the last linear layer, the softmax function is defined as:\n",
    "\n",
    "$$ P(y = i) = \\frac{e^{Z_i}}{\\sum_{j=1}^Ke^{Z_j}} , i \\in \\{0, \\cdots, K-1 \\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy in numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9729189131256584\n"
     ]
    }
   ],
   "source": [
    "y = np.array([1, 0, 0])\n",
    "z = np.array([0.2, 0.1, -0.1])\n",
    "y_pred = np.exp(z) / np.sum(np.exp(z))\n",
    "loss = np.sum(-y * np.log(y_pred))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9729)\n"
     ]
    }
   ],
   "source": [
    "y = torch.LongTensor([0])\n",
    "z = torch.FloatTensor([[0.2, 0.1, -0.1]])\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "loss = criterion(z, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-1 Prepare Dataset\n",
    "\n",
    "Note the use of `transforms.ToTensor()` to convert the images from PIL format to Tensor format.\n",
    "$$ \\mathbb Z^{28 \\times 28}, pixel \\in \\{0, \\cdots, 255 \\} \\rightarrow \\mathbb Z^{1 \\times 28 \\times 28}, pixel \\in [0, 1] $$\n",
    "\n",
    "And we use `transforms.Normalize()` to normalize the data in this way:\n",
    "$$ Pixel_{norm} = \\frac{Pixel_{origin} - mean}{std} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                      # Convert PIL image to PyTorch Tensor.\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Normalize using the mean value and std loss value\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./dataset/mnist', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./dataset/mnist', train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-2 Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(28 * 28, 512)\n",
    "        self.layer2 = torch.nn.Linear(512, 256)\n",
    "        self.layer3 = torch.nn.Linear(256, 128)\n",
    "        self.layer4 = torch.nn.Linear(128, 64)\n",
    "        self.layer5 = torch.nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = f.relu(self.layer1(x))\n",
    "        x = f.relu(self.layer2(x))\n",
    "        x = f.relu(self.layer3(x))\n",
    "        x = f.relu(self.layer4(x))\n",
    "        return self.layer5(x)\n",
    "    \n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-3 Define Loss and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)    # 最后一个参数是冲量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-4 Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 2.295\n",
      "[1,   200] loss: 2.264\n",
      "[1,   300] loss: 2.114\n",
      "[1,   400] loss: 1.538\n",
      "[1,   500] loss: 0.890\n",
      "[1,   600] loss: 0.620\n",
      "[1,   700] loss: 0.480\n",
      "[1,   800] loss: 0.442\n",
      "[1,   900] loss: 0.412\n",
      "[2,   100] loss: 0.359\n",
      "[2,   200] loss: 0.340\n",
      "[2,   300] loss: 0.308\n",
      "[2,   400] loss: 0.297\n",
      "[2,   500] loss: 0.261\n",
      "[2,   600] loss: 0.264\n",
      "[2,   700] loss: 0.240\n",
      "[2,   800] loss: 0.233\n",
      "[2,   900] loss: 0.223\n",
      "[3,   100] loss: 0.209\n",
      "[3,   200] loss: 0.189\n",
      "[3,   300] loss: 0.184\n",
      "[3,   400] loss: 0.173\n",
      "[3,   500] loss: 0.176\n",
      "[3,   600] loss: 0.158\n",
      "[3,   700] loss: 0.153\n",
      "[3,   800] loss: 0.149\n",
      "[3,   900] loss: 0.142\n",
      "[4,   100] loss: 0.142\n",
      "[4,   200] loss: 0.129\n",
      "[4,   300] loss: 0.126\n",
      "[4,   400] loss: 0.130\n",
      "[4,   500] loss: 0.135\n",
      "[4,   600] loss: 0.121\n",
      "[4,   700] loss: 0.109\n",
      "[4,   800] loss: 0.101\n",
      "[4,   900] loss: 0.109\n",
      "[5,   100] loss: 0.095\n",
      "[5,   200] loss: 0.098\n",
      "[5,   300] loss: 0.092\n",
      "[5,   400] loss: 0.097\n",
      "[5,   500] loss: 0.108\n",
      "[5,   600] loss: 0.086\n",
      "[5,   700] loss: 0.099\n",
      "[5,   800] loss: 0.092\n",
      "[5,   900] loss: 0.088\n",
      "[6,   100] loss: 0.072\n",
      "[6,   200] loss: 0.070\n",
      "[6,   300] loss: 0.082\n",
      "[6,   400] loss: 0.078\n",
      "[6,   500] loss: 0.077\n",
      "[6,   600] loss: 0.084\n",
      "[6,   700] loss: 0.074\n",
      "[6,   800] loss: 0.073\n",
      "[6,   900] loss: 0.061\n",
      "[7,   100] loss: 0.056\n",
      "[7,   200] loss: 0.064\n",
      "[7,   300] loss: 0.059\n",
      "[7,   400] loss: 0.058\n",
      "[7,   500] loss: 0.058\n",
      "[7,   600] loss: 0.063\n",
      "[7,   700] loss: 0.059\n",
      "[7,   800] loss: 0.058\n",
      "[7,   900] loss: 0.067\n",
      "[8,   100] loss: 0.043\n",
      "[8,   200] loss: 0.049\n",
      "[8,   300] loss: 0.048\n",
      "[8,   400] loss: 0.054\n",
      "[8,   500] loss: 0.055\n",
      "[8,   600] loss: 0.055\n",
      "[8,   700] loss: 0.049\n",
      "[8,   800] loss: 0.049\n",
      "[8,   900] loss: 0.045\n",
      "[9,   100] loss: 0.039\n",
      "[9,   200] loss: 0.036\n",
      "[9,   300] loss: 0.038\n",
      "[9,   400] loss: 0.042\n",
      "[9,   500] loss: 0.045\n",
      "[9,   600] loss: 0.042\n",
      "[9,   700] loss: 0.041\n",
      "[9,   800] loss: 0.044\n",
      "[9,   900] loss: 0.037\n",
      "[10,   100] loss: 0.026\n",
      "[10,   200] loss: 0.032\n",
      "[10,   300] loss: 0.033\n",
      "[10,   400] loss: 0.033\n",
      "[10,   500] loss: 0.032\n",
      "[10,   600] loss: 0.028\n",
      "[10,   700] loss: 0.037\n",
      "[10,   800] loss: 0.033\n",
      "[10,   900] loss: 0.042\n",
      "Accuracy on test set:  97.5900%\n",
      "[11,   100] loss: 0.022\n",
      "[11,   200] loss: 0.030\n",
      "[11,   300] loss: 0.028\n",
      "[11,   400] loss: 0.027\n",
      "[11,   500] loss: 0.023\n",
      "[11,   600] loss: 0.029\n",
      "[11,   700] loss: 0.032\n",
      "[11,   800] loss: 0.029\n",
      "[11,   900] loss: 0.022\n",
      "[12,   100] loss: 0.022\n",
      "[12,   200] loss: 0.021\n",
      "[12,   300] loss: 0.017\n",
      "[12,   400] loss: 0.017\n",
      "[12,   500] loss: 0.024\n",
      "[12,   600] loss: 0.017\n",
      "[12,   700] loss: 0.028\n",
      "[12,   800] loss: 0.022\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m---> 29\u001b[0m         \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m9\u001b[39m:\n\u001b[0;32m     31\u001b[0m             test()\n",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(epoch: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m      2\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (inputs, target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m      4\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      6\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n",
      "File \u001b[1;32mc:\\Users\\jerry\\miniconda3\\envs\\vpteam\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\jerry\\miniconda3\\envs\\vpteam\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\jerry\\miniconda3\\envs\\vpteam\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\jerry\\miniconda3\\envs\\vpteam\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\jerry\\miniconda3\\envs\\vpteam\\lib\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\jerry\\miniconda3\\envs\\vpteam\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\jerry\\miniconda3\\envs\\vpteam\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jerry\\miniconda3\\envs\\vpteam\\lib\\site-packages\\torchvision\\transforms\\functional.py:173\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    171\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m1\u001b[39m], pic\u001b[38;5;241m.\u001b[39msize[\u001b[38;5;241m0\u001b[39m], F_pil\u001b[38;5;241m.\u001b[39mget_image_num_channels(pic))\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# put it from HWC to CHW format\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mdefault_float_dtype)\u001b[38;5;241m.\u001b[39mdiv(\u001b[38;5;241m255\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(epoch: int):\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (inputs, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 100 == 99:\n",
    "            print(f'[{epoch + 1}, {batch_idx + 1:5d}] loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "def test():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for (images, labels) in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f'Accuracy on test set: {100 * correct / total: .4f}%')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for epoch in range(10):\n",
    "        train(epoch)\n",
    "        if epoch % 10 == 9:\n",
    "            test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作业\n",
    "\n",
    "- [Cross Entropy Loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) 和 [NLL Loss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss) 的区别\n",
    "- 尝试理解: Cross Entropy Loss $\\Leftrightarrow$ Log Softmax + NLL Loss\n",
    "- 完成Kaggle中的[Otto Group Product Classification任务](https://www.kaggle.com/c/otto-group-product-classification-challenge/data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding\n",
    "\n",
    "Cross Entropy Loss 和 NLL Loss 都是常用的分类任务的损失函数，它们的区别在于 Cross Entropy Loss 内部包含了 softmax 和 log 操作，而 NLL Loss 只是对输入的对数概率向量和目标标签进行负对数似然计算¹²。也就是说，Cross Entropy Loss = NLL Loss + softmax + log。因此，如果使用 Cross Entropy Loss 作为损失函数，神经网络的最后一层就不需要加入 softmax 或者 log softmax，而如果使用 NLL Loss，就需要在最后一层加入 log softmax³⁴。下面是一个简单的例子，说明两种损失函数的计算过程：\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 随机生成一个神经网络的最后一层，3行4列，那就是有4个标签\n",
    "input = torch.randn(3, 4)\n",
    "# input的第一行设置为标签1，第二行为标签0, 第三行为标签2\n",
    "label = torch.tensor([1, 0, 2])\n",
    "\n",
    "# 定义损失函数为NLLLoss\n",
    "loss = nn.NLLLoss()\n",
    "# 定义log softmax函数，也就是将input中的每一行转化为带有负号的数字\n",
    "m = nn.LogSoftmax(dim=1)\n",
    "# 计算损失，损失就是一个值。\n",
    "loss_value = loss(m(input), label)\n",
    "print(loss_value) # tensor(1.7075)\n",
    "\n",
    "# 定义损失函数为CrossEntropyLoss\n",
    "loss = nn.CrossEntropyLoss()\n",
    "# 计算损失，损失就是一个值。\n",
    "loss_value = loss(input, label)\n",
    "print(loss_value) # tensor(1.7075)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "可以看到，两种损失函数的结果是一样的，只是 CrossEntropyLoss 不需要额外的 log softmax 操作。⁵\n",
    "\n",
    "Source: Conversation with Bing, 2/21/2024 \\\n",
    "(1) 神经网络中NLLLoss和CrossEntropyLoss的快速理解 - 知乎. https://zhuanlan.zhihu.com/p/589631793. \\\n",
    "(2) 详解pytorch的损失函数：NLLLoss()和CrossEntropyLoss() - 知乎. https://zhuanlan.zhihu.com/p/570118948. \\\n",
    "(3) 详解pytorch的损失函数：NLLLoss()和CrossEntropyLoss() - 知乎. https://bing.com/search?q=Cross+Entropy+Loss+%e5%92%8c+NLL+Loss+%e7%9a%84%e5%8c%ba%e5%88%ab. \\\n",
    "(4) pytorch中F.cross_entropy和F.nll_loss的区别-CSDN博客. https://blog.csdn.net/code_plus/article/details/115481575. \\\n",
    "(5) pytorch中CrossEntropyLoss和NLLLoss的区别与联系 - CSDN博客. https://blog.csdn.net/qq_25105061/article/details/107381316."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vpteam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
